{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse du sentiment (Imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy\n",
    "# Inutile :\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation et codage des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('imdb_train.csv')\n",
    "data_test = pd.read_csv('imdb_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, I know I shouldn't like this movie but I do. From Pat Morita's loveable interpretation of a Japanese stereotype to Jay Leno's annoying yell, I laughed throughout this movie.As long as you take into account that this is not the best movie in the world, it's a good mvie.<br /><br />My favorite part is Morita talking to his boss in Tokyo with the drinking a close second.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(data_train.Review[1])\n",
    "print(data_train.Eval[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On sauvegarde les phrases avant transformation dans R_train, qu'on utilisera plus tard pour vérifier les prédictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R_train = data_train.Review\n",
    "Y_train = data_train.Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On utilise le *tokeniser* de Keras pour convertir les phrases en séquences de nombres (équivalent au codage avec *one_hot*)  \n",
    "On retient les 5000 mots qui apparaissent le plus souvent (*top_words*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "\n",
    "top_words = 5000\n",
    "tokenizer = Tokenizer(num_words=top_words, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(R_train)\n",
    "X_train = tokenizer.texts_to_sequences(R_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(R_train[1])\n",
    "print(X_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On transforme de même les données de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_test = data_test.Review\n",
    "Y_test = data_test.Eval\n",
    "X_test = tokenizer.texts_to_sequences(R_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On complète les séquences avec des 0 à gauche pour avoir des séquences de même longueur (ici 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words, padding='post')\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(X_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réseaux denses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence avec un modèle dense simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apprentissage\n",
    "model.fit(X_train, Y_train, epochs=2, batch_size=128, verbose=2)\n",
    "# Evaluation\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On calcule les prédictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_mlp = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test  \n",
    "si prediction proche de 0 : avis négatif  \n",
    "si prediction proche de 1 : avis positif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = random.randint(1, X_test.shape[0])\n",
    "print(R_test[ir])\n",
    "print(Y_mlp[ir])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réseaux convolutionnels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On utilise une couche convolutionnelle à une dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Conv1D(filters=32, kernel_size=10, input_shape=(32, 1), activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apprentissage\n",
    "model.fit(X_train, Y_train, epochs=10, batch_size=128, verbose=2)\n",
    "# Evaluation\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_cnn1 = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ir = random.randint(1, X_test.shape[0])\n",
    "print(R_test[ir])\n",
    "print(Y_cnn1[ir])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec un réseau un peu plus complexe :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 128, input_length=max_words))\n",
    "model.add(Conv1D(filters=128, kernel_size=10, input_shape=(128, 1), activation=\"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apprentissage\n",
    "model.fit(X_train, Y_train, epochs=10, batch_size=128, verbose=2)\n",
    "# Evaluation\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_cnn2 = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ir = random.randint(1, X_test.shape[0])\n",
    "print(R_test[ir])\n",
    "print(Y_cnn2[ir])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réseaux récurrents (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention :l'apprentissage avec des LSTM peut être assez long ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apprentissage\n",
    "model.fit(X_train, Y_train, epochs=10, batch_size=128, verbose=2)\n",
    "# Evaluation\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les installation avec *Tensorflow GPU*, on peut utiliser *CuDNNLSTM*, beaucoup plus rapide :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import CuDNNLSTM, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 100, input_length=max_words))\n",
    "model.add(CuDNNLSTM(128))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apprentissage\n",
    "model.fit(X_train, Y_train, epochs=10, batch_size=128, verbose=2)\n",
    "# Evaluation\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice\n",
    "\n",
    "Tester les différentes approches sur le dataset des Newsgroups Reuters  \n",
    "https://keras.io/datasets/#reuters-newswire-topics-classification  \n",
    "  \n",
    "  \n",
    "Il s'agit de classer les news dans un des 46 groupes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from keras.datasets import reuters  \n",
    "top_words = 30000  \n",
    "(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La lecture directe du dataset ne nécessite pas la phase de tokenisation/codage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Indication** : on doit ici réaliser une classification dans des catégories multiples (un peu différent de la décision 0/1 de l'exemple Imdb). On pourra s'inspirer de l'exemple *mnist*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
